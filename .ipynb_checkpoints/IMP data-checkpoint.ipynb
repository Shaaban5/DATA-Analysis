{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First\n",
    "=====\n",
    "\n",
    "Second\n",
    "------\n",
    "\n",
    "# Header 1\n",
    "## Header 2\n",
    "### Header 3\n",
    "\n",
    "[for more data](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) here **i'am** _just_ __looking__ around \n",
    "[list of magic words](https://ipython.readthedocs.io/en/stable/interactive/magics.html)\n",
    "$$\n",
    "y = \\frac{a}{b+c}\n",
    "$$\n",
    "> **Another test:** Click on this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Jupyter:\n",
    "=======\n",
    "- jupyter nbconvert --to html notebook.ipynb   \\convert html to .ipynb [check doc](https://nbconvert.readthedocs.io/en/latest/usage.html)\n",
    "- jupyter nbconvert notebook.ipynb --to slides \\convert and immediately see jupyter nbconvert notebook.ipynb --to slides --post serve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Conda** :\n",
    "\n",
    "- conda upgrade conda\n",
    "- conda upgrade --all\n",
    "- conda create –n project_name python=3\n",
    "- conda create -n env_name list of packages \\ex: conda create -n data python=3.6 numpy pandas\n",
    "- conda env export > environment.yaml\n",
    "- conda env create -f environment.yaml\n",
    "- conda env remove -n env_name\n",
    "- activate project_name \\deactivate\n",
    "- conda env list\n",
    "- conda install numby panda matplotlib\n",
    "- conda install jupyter notebook\n",
    "- conda remove package_name=1.0.2 \n",
    "- conda update package_name\n",
    "- conda update –all\n",
    "- conda list\n",
    "- conda search *search_term*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Panda:\n",
    "======\n",
    "\n",
    "- **df = pd.read_csv**('student_scores.csv', sep=',' , header = 0 or None or any int, names = create list with columns, index_col = 'column name to be the index' or more than one by creating list , index = False to cancel the index num if u have one,  ) start reading\n",
    "- **df.head(5)**   return first 5 rows along with the header, or set how much to return.\n",
    "- **df.tail(5)**   return last 5 rows along with the header, or to set how much to return.\n",
    "- **df.shape**    return tuple ( rows count, columns count).\n",
    "- **df.dtypes**    return the data type of each columns(bool, int, float, object).\n",
    "- **type(df['column name'][0])** return the excatly data type of data(str, ....).\n",
    "- **df.info()**  return summary of datafram and number of non-null values in each column.\n",
    "- **df.nunique()**    return number of unique values in each column & **df.unique()** return list if that values.\n",
    "- **df.column_name.nunique()** return the no of unique in singal column (write without ' ').\n",
    "- **df.describe()**    return seful descriptive statistics for each column of data.\n",
    "- **df_selecting = df.iloc**[ form row index : to row index , from column index : to column index] \n",
    "- **df_selecting = df.loc**[ form row index : to row index , from column 'name' : to column 'name' ]\n",
    "- to select different rows or columns use   **np.r_[ : 2 , 10, 12 : 22]**    but import numpy first.\n",
    "- **df.columns** = new_labels to change the columns name .\n",
    "\n",
    "sample to print the columns with its index# for slicing:\n",
    "\n",
    "> - for i, v in enumerate(df.columns):\n",
    ">     - print(i, v)\n",
    "\n",
    "- Fill the null values with mean:**1-** mean = df['column name'].mean().**2-** df['column nam'].fillna(mean, inplace = True) or assign it to the original df['coumn name'] = df['coulmn name'].fillna(mean)\n",
    "\n",
    "- **df.duplicates()**    return all row with True or False (check that all data are the same) , **sum(df.duplicated())** return total identical rows.\n",
    "- **df.drop_duplicated**(inplace = True) to del the identical rows.\n",
    "- df['column time'] = **pd.to_datetime**(df['column time']) .\n",
    "\n",
    "\n",
    "- **df.hist() ;** figsize=(int, int) also could be end by **;** to view histograms for all the numerical columns, or **df['column name'].hist();** to view a spcifice column.\n",
    "> can use **df['column name'].plot(kind = 'hist');**  kind = hist, bar, pie, scatter, box.\n",
    "- for non numercial columns, use **df['column name'].value_counts()** to count each repeated value and return it.\n",
    "> can call plot directly **df['column name'].value_counts().plot(kind = 'bar', figsize = (8 ,8));**\n",
    "    - to set index: ind = **df['column u need data'].value_counts().index** used in plotting\n",
    "    - example: **df['column name'].value_counts()[ind].plot(kind = 'bar', figsize = (8 ,8));**\n",
    "\n",
    "- **pd.plotting.scatter_matrix(df, figsize=(15,15));** to watch matrix of all numercial columns (for quick insight).\n",
    "- 2 columns **df.plot(x= column name , y= other coulmn name , kind = 'scatter')**\n",
    "- seperate group of df(by assign each to var) and use them visualise and compare to another data in df\n",
    "\n",
    "    Create your own data frame:\n",
    "    - df_m = **df[df['from column name'] == 'm']** assigning specific data or **df.query(' from column name == \"m\" ' )**\n",
    "    - mask = **df['from column name'] == 'm'**  return serious of boolean weather all columns equal to certain value.\n",
    "    - **df_m = df[mask]** is another way to assign all True from mask to df_m.\n",
    "    - last_month = **df[df['week']>= '02-01-2018']**  assinging last month data as example......\n",
    "    - **last_month.append(df.sum(numeric_only=True), ignore_index=True)**   here sum the total\n",
    "    - or **df.iloc[int: , 1: ].sum()** here we selected the required data and we skip the non numerical\n",
    "    - using **df[df['column'] == df['column'].min()] or max() or sum() mean()..**\n",
    "    - by index **df[ df.index >= 'value u need to start' ]**\n",
    "    - return duplicaed row **df.loc[ df.duplicated() , : ]** or  **df.loc[df['data in column'].unnique() , : ]**\n",
    "    - append columns: **df['new column'] = array** , **array = np.repeat( value, int for how many)**\n",
    "\n",
    "- Change column name **df.rename**((**index**=str, **columns**={ from \"A\": to \"a\"}, **axis**='columns or index' **inpalce** = True)\n",
    "- Saving file **df.to_csv('Path' , sep = ',' , index = False)** \n",
    "\n",
    "    *Grouping*:\n",
    "    -----------\n",
    "- **df.groupby('column').mean()** column will be the index, or **groupby(['column1', 'column2'])** or **(as_index = False)** to cancel column to be index, or **groupby(...)['display specific column']**\n",
    "\n",
    "    *CUTTing*:\n",
    "    ---------- \n",
    "divid column into pieces as per givin labels\n",
    "- set bins = [min , 25% , 50% , 75% , max ]\n",
    "- set labels=[ ' G1 ' , ' G2 ' ,' G3 ' , ' G4 '  ]\n",
    "- df['new column name'] = df.cut(df['column needed'] , bins , labels = labels_list)\n",
    "\n",
    "[Check this 1](http://nbviewer.jupyter.org/format/slides/github/jorisvandenbossche/2015-PyDataParis/blob/master/pandas_introduction.ipynb#/)\n",
    "\n",
    "[Check this 2](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Standard and other important lib:\n",
    "=================================\n",
    "\n",
    "- csv: very convenient for reading and writing csv files\n",
    "- collections: useful extensions of the usual data types including OrderedDict, defaultdict and namedtuple\n",
    "- random: generates pseudo-random numbers, shuffles sequences randomly and chooses random items\n",
    "- string: more functions on strings. This module also contains useful collections of letters like string.digits (a string containing all characters with are valid digits).\n",
    "- re: pattern-matching in strings via regular expressions\n",
    "- math: some standard mathematical functions\n",
    "- os: interacting with operating systems\n",
    "- os.path: submodule of os for manipulating path names\n",
    "- sys: work directly with the Python interpreter\n",
    "- json: good for reading and writing json files (good for web work)\n",
    "\n",
    "\n",
    "- IPython - A better interactive Python interpreter\n",
    "- requests - Provides easy to use methods to make web requests. Useful for accessing web APIs.\n",
    "- Flask - a lightweight framework for making web applications and APIs.\n",
    "- Django - A more featureful framework for making web applications. Django is particularly good for designing complex, content heavy, web applications.\n",
    "- Beautiful Soup - Used to parse HTML and extract information from it. Great for web scraping.\n",
    "- pytest - extends Python's builtin assertions and unittest module.\n",
    "- PyYAML - For reading and writing YAML files.\n",
    "- NumPy - The fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object and useful linear algebra capabilities.\n",
    "- pandas - A library containing high-performance, data structures and data analysis tools. In particular, pandas provides dataframes!\n",
    "- matplotlib - a 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments.\n",
    "- ggplot - Another 2D plotting library, based on R's ggplot2 library.\n",
    "- Pillow - The Python Imaging Library adds image processing capabilities to your Python interpreter.\n",
    "- pyglet - A cross-platform application framework intended for game development.\n",
    "- Pygame - A set of Python modules designed for writing games.\n",
    "- pytz - World Timezone Definitions for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  342,   346,   371,   370,   458,   439,   376,   427,   351,\n",
       "              325,\n",
       "            ...\n",
       "             6238, 14016,  8031,  5316, 89719,  7365,  3271,  3527,  4958,\n",
       "            10581],\n",
       "           dtype='int64', length=5734)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\Home\\Downloads\\NYC-CitiBike-2016.csv')\n",
    "df['tripduration'].value_counts().index"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

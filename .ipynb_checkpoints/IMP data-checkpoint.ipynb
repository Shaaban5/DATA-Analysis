{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First\n",
    "=====\n",
    "\n",
    "Second\n",
    "------\n",
    "\n",
    "# Header 1\n",
    "## Header 2\n",
    "### Header 3\n",
    "\n",
    "[for more data](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) here **i'am** _just_ __looking__ around \n",
    "[list of magic words](https://ipython.readthedocs.io/en/stable/interactive/magics.html)\n",
    "$$\n",
    "y = \\frac{a}{b+c}\n",
    "$$\n",
    "> **Another test:** Click on this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Jupyter:\n",
    "=======\n",
    "- jupyter nbconvert --to html notebook.ipynb   \\convert html to .ipynb [check doc](https://nbconvert.readthedocs.io/en/latest/usage.html)\n",
    "- jupyter nbconvert notebook.ipynb --to slides \\convert and immediately see jupyter nbconvert notebook.ipynb --to slides --post serve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Conda** :\n",
    "\n",
    "- conda upgrade conda\n",
    "- conda upgrade --all\n",
    "- conda create –n project_name python=3\n",
    "- conda create -n env_name list of packages \\ex: conda create -n data python=3.6 numpy pandas\n",
    "- conda env export > environment.yaml\n",
    "- conda env create -f environment.yaml\n",
    "- conda env remove -n env_name\n",
    "- activate project_name \\deactivate\n",
    "- conda env list\n",
    "- conda install numby panda matplotlib\n",
    "- conda install jupyter notebook\n",
    "- conda remove package_name=1.0.2 \n",
    "- conda update package_name\n",
    "- conda update –all\n",
    "- conda list\n",
    "- conda search *search_term*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Panda:\n",
    "======\n",
    "\n",
    "- **df = pd.read_csv**('student_scores.csv', sep=',' , header = 0 or None or any int, names = create list with columns, index_col = 'column name to be the index' or more than one by creating list , index = False to cancel the index num if u have one,  ) start reading\n",
    "- **df.head(5)**   return first 5 rows along with the header, or set how much to return.\n",
    "- **df.tail(5)**   return last 5 rows along with the header, or to set how much to return.\n",
    "- **df.shape**    return tuple ( rows count, columns count) also **dg.shape[0]** as total rows\n",
    "- **df.dtypes**    return the data type of each columns(bool, int, float, object).\n",
    "        use type(df.column[1]) or type(df['column'][1]\n",
    "        \n",
    "- **df.copy(*deep* = *True* default or *False* both copy reflect any change in one of them)**\n",
    "- **type(df['column name'][0])** return the excatly data type of data(str, ....).\n",
    "- **df.col.sort_value()** to sort the values\n",
    "- **df.info()**  return summary of datafram and number of non-null values in each column.\n",
    "- **df.nunique()**    return number of unique values in each column & **df.unique()** return list of that values.\n",
    "- **df.column_name.nunique()** return the no of unique in singal column (write without ' ').\n",
    "- **df.describe()**    return seful descriptive statistics for each column of data:\n",
    "            df.describe()['min or 25% or 50% or 75% or max] return only required value\n",
    "            use list(df.describe()['count','mean', 'std' ,'min' : 'max']) or ['min': ] to make a list with values\n",
    "            use df.describe()['min' : 'min'] if there are servral columns aslo add ['column'] to select specific 1\n",
    "- **df_selecting = df.iloc**[ form row index : to row index , from column index : to column index] \n",
    "- **df_selecting = df.loc**[ form row index : to row index , from column 'name' : to column 'name' ]\n",
    "        use multiple condition as follow q1_df.loc[(0.97 <= df.col1) & (df.col1 <= 1.04) , 'col2'] = 'str' (& is AND )(| is or)\n",
    "- to select different rows or columns use   **np.r_[ : 2 , 10, 12 : 22]**    but import numpy first.\n",
    "- **df.isnull()** print boolean for all df null True / **df.isnul().sum()** for total counts\n",
    "        df.isnull().any(axis=1).sum() total count of rows with null values \n",
    "        any() return boolean results / axis=1 to select the rows instead of columns\n",
    "- **df.columns** = new_labels to change the columns name .\n",
    "        check if 2 df are the same columns for appending (df_1.columns == df_2.columns).all() one result True or False / or df_1.columns == df_2.columns result a list of True or False for each column\n",
    "- SWITCH rows and columns **df.transpose()** \n",
    "- change df to serise by using **iloc[:, :]**\n",
    "\n",
    "sample to print the columns with its index# for slicing:\n",
    "\n",
    "> - for i, v in enumerate(df.columns):\n",
    ">     - print(i, v)\n",
    "\n",
    "- Fill the null values with mean:\n",
    "            1- mean = df['column name'].mean().\n",
    "            2- df['column nam'].fillna(mean, inplace = True) \n",
    "                or assign it to the original df['coumn name'] = df['coulmn name'].fillna(mean)\n",
    "\n",
    "- **df.duplicates()**    return all row with True or False (check that all data are the same):\n",
    "            **df.duplicated().sum()** or **sum(df.duplicated())** return total identical rows.\n",
    "- **df.drop_duplicates**(inplace = True) to del the identical rows.\n",
    "- df['column time'] = **pd.to_datetime**(df['column time']) .\n",
    "- **df.Series.dt.year** or day or.. to **extract** year or month ex: **df.date_col.dt.year**\n",
    "        2 time formate **datetime** as 2017-01-01 & **timedelta** 293 days \n",
    "        to change timedelta to num of years or month or .. use np.timedelta64(1, 'D') or 'Y' or 'M' or ..\n",
    "\n",
    "- **pd.to_numeric**(*arg*, *downcast*={‘integer’, ‘signed’, ‘unsigned’, ‘float’}, *errors* = {‘ignore’, ‘raise’, ‘coerce’})\n",
    "        or df_18['column'].astype(int) pr astype(float)\n",
    "- drop rows or columns **df.drop**(*labels*=list, *axis*=0, *index*=None, *columns*=list, *level*=None, *inplace*=False)\n",
    "        df.drop(selected_df.index, inplace=True)\n",
    "- drop null rows **df.dropna**(*axis*=0 'index or 1 'column', *how*='any' with null or 'all' only row with all null , *thresh*=None or drop at least # of null in each row or column, subset=None or define which column to look at, *inplace*=False)\n",
    "\n",
    "    *Plotting*:\n",
    "    ----------\n",
    "- **df.hist() ;** figsize=(int, int) optional end by **;** to view histograms for all the numerical columns, or **df['column name'].hist();** to view a spcifice column.\n",
    "> can use **df['column name'].plot(kind = 'hist');**  kind = hist, bar, pie, scatter, box / title='str' /color= 'red' or ['red', 'blue'] / alpha = 0.7\n",
    "- **pd.plotting.scatter_matrix(df, figsize=(15,15));** to watch matrix of all numercial columns (for quick insight).\n",
    "- close results needs **df.plot.line()** show the difference in lines and \n",
    "- 2 columns **df.plot(x= column name , y= other coulmn name , kind = 'scatter')**\n",
    "- seperate group of df(by assign each to var) and use them visualise and compare to another data in df\n",
    "- for non numercial columns, use **df['column name'].value_counts()** to count each repeated value and return it.\n",
    "> can call plot directly **df['column name'].value_counts().plot(kind = 'bar', figsize = (8 ,8));**\n",
    "    - to set index: ind = **df['column u need data'].value_counts().index** used in plotting\n",
    "    - example: **df['column name'].value_counts()[ind].plot(kind = 'bar', figsize = (8 ,8));**\n",
    "- **df.sample(200)** choice sample from df\n",
    "\n",
    "\n",
    "   *Create your own data frame:*\n",
    "    ------------------------------\n",
    "- df_m = **df[df['from column name'] == 'm']** assigning specific data or **df.query(' from column name == \"m\" ' )**:\n",
    "            df.query('column name > {} '.format(df[''].median()) example of using format() in query\n",
    "            df.query('column in [list of several values])\n",
    "            and could be used as sub query of query df.query('col1 == condition').query('col2 == condition2)\n",
    "- mask = **df['from column name'] == 'm'**  return serious of boolean weather all columns equal to certain value.\n",
    "- **df_m = df[mask]** is another way to assign all True from mask to df_m.\n",
    "- last_month = **df[df['week']>= '02-01-2018']**  assinging last month data as example......\n",
    "- **last_month.append(df.sum(numeric_only=True), ignore_index=True)**   here sum the total\n",
    "    - or **df.iloc[int: , 1: ].sum()** here we selected the required data and we skip the non numerical\n",
    "- using **df[df['column'] == df['column'].min()]** or max() or sum() mean() median() count().\n",
    "- by index **df[ df.index >= 'value u need to start' ]**\n",
    "- return duplicated row **df.loc[ df.duplicated() , : ]** or  **df.loc[df['data in column'].unnique() , : ]**\n",
    "- add new or update columns: **df['new column'] = array** , **array = np.repeat( value, int for how many)**\n",
    "\n",
    "- [rename](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html) column name **df.rename**(**index**=str, **columns**={ from \"A\": to \"a\"}, **axis**='columns or index' **inpalce** = True)\n",
    "        df.rename(columns=lambda x: x[:10]+'str') example\n",
    "- apply func to all row or column **df.apply(func, axis= {0 or ‘index’, 1 or ‘columns’} default 0)**\n",
    "- Saving file **df.to_csv('Path' , sep = ',' , index = False)** \n",
    "\n",
    "    *Grouping*:\n",
    "    -----------\n",
    "- **df.groupby('column').mean()** column will be the index\n",
    "- **df.groupby(['column1', 'column2'])** or **(as_index = False)** to cancel column to be index, or **groupby(...)['display specific column']**:\n",
    "            or df.groupby('column to grouped').mean().column_name to be returned along.\n",
    "\n",
    "    *CUTTing*:\n",
    "    ---------- \n",
    "divid column into pieces as per givin labels\n",
    "- set bins = [min , 25% , 50% , 75% , max ]\n",
    "- set labels=[ ' G1 ' , ' G2 ' ,' G3 ' , ' G4 '  ]\n",
    "- df['new column name'] = **pd**.cut(df['column needed'] , bins , labels = labels_list)\n",
    "\n",
    "##[String handling](https://pandas.pydata.org/pandas-docs/stable/api.html#string-handling):\n",
    "- **df.str.extract**('similar str to original and add expression of what u need' , *expand*= True return df or False Series  \n",
    "- **df.str.contains('/')** return all data that contain a given charachters\n",
    "\n",
    "expressions:\n",
    "- \\w any single letter, digit or underscore\n",
    "- \\W any charchter not part of \\w\n",
    "- \\s space, newline, tab, return\n",
    "- \\S any character not part of \\s \n",
    "- \\t tab - \\n newline - \\r return\n",
    "- \\d decimal digit 0-9 & use \\d+ to collect next numbers\n",
    "- ^  pattern at the start of the str - **dollar signe** pattern at the end of str\n",
    "- [abc] match a or b or c\n",
    "- [a-zA-Z0-9] (a to z) or (A to Z) or (0 to 9)\n",
    "\n",
    "\n",
    "- [concat](https://pandas.pydata.org/pandas-docs/stable/merging.html#concatenating-objects) pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True)\n",
    "- [merge](https://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging) pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True,suffixes=('_x', '_y'), copy=True, indicator=False,validate=None)\n",
    "        combined_df = pd.merge(left_df, right_df, left_on=  , right_on = )\n",
    "                or = df_left.merge(df_right, left_on , right_on)\n",
    "\n",
    "[Check this 1](http://nbviewer.jupyter.org/format/slides/github/jorisvandenbossche/2015-PyDataParis/blob/master/pandas_introduction.ipynb#/)\n",
    "\n",
    "[Check this 2](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python)\n",
    "\n",
    "[Sammary sheet](https://www.dataquest.io/blog/pandas-cheat-sheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "matplotlib\n",
    "========\n",
    "\n",
    "- set plot title **plt.title('str')**\n",
    "- set X name **plt.xlabel('title' , fontsize = 12)** / Y name **plt.ylabel('title', 12)**\n",
    "- customizing **plt.bar**(*left*: [ticks loc seq] , *height*: [bars values], *width*: widths bar , *bottom*: y coor, *color*, *edgecolor* , *linewidth* , *tick_label*: tick label, *orientation*: bars orientation , *log*: boolean)\n",
    "        left = ['1', '5'] as 2 locations or [1,5] as 5 locations then inster location names by tick_label\n",
    "        df2 =df.groupby('column1').column2.mean()\n",
    "        plt.bar(df2.index , df2 )\n",
    "        plt.subplots(figsize=(8, 5))\n",
    "\n",
    "- x ticks names **plt.xticks(ticks, label )** ticks=list of positions 1,2,3 or 3,1,2 labels= list of label for location:\n",
    "        locs, labels = xticks()  # Get locations and labels\n",
    "        xticks(ticks, [labels])  # Set locations and labels\n",
    "- line plot **plt.plot( x: seq , y: seq )**\n",
    "\n",
    "- plt.axvline(x= np.percentile(data, 2.5) , color= , linewidth= )\n",
    "- plt.axvline(x= np.percentile(data, 97.5) , color= , linewidth= )\n",
    "\n",
    "\n",
    "example:\n",
    "---------\n",
    "### plot bars\n",
    "- red_bars = plt.bar(ind, red_proportions, width, color='r', alpha=.7, label='Red Wine')\n",
    "- white_bars = plt.bar(ind + width, white_proportions, width, color='w', alpha=.7, label='White Wine')\n",
    "        red_proportion is serise, if not use red_proportion.column that u need\n",
    "        ind is array of num inorder to +width\n",
    "\n",
    "### title and labels\n",
    "- plt.sunplots(figsize = (8 width ,5 hieght)\n",
    "- plt.ylabel('Proportion')\n",
    "- plt.xlabel('Quality')\n",
    "- locations = ind + width/2  # xtick locations\n",
    "- labels = ['3', '4', '5', '6', '7', '8', '9']  or **serise.index** # xtick labels\n",
    "- plt.xticks(locations, labels)\n",
    "\n",
    "### legend\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Numpy:\n",
    "======\n",
    "\n",
    "- np.random.randint(low inclusive, high exclusive, size= size to display or (size, count in each cell, dtype= int64, int...)\n",
    "    - to select specific data it could sum(axis=1) to sum each cell then ex: (variable1.sum(axis=1) == 1).mean() \n",
    "    - size could be (int(1e6) as 1 million, 4 as 4 results in each cell)\n",
    "\n",
    "- np.randon.choice(a 'array', size , replace= True or false , replace= False if u need unique elements only, P= [probability of each choice] in order)\n",
    "- np.random.binomial(a trials count int or array, p probaility in each trial float or array, size as above) return # of success\n",
    "- np.**random.seed**(a_fixed_number) every time you call the numpy's other random function, the result will be the same\n",
    "- np.random.normal(loc: mean, scale: std, size) \n",
    "\n",
    "\n",
    "- np.var variance/ np.std standard deviation (a, axis=None, dtype=None, out=None, ddof=0, keepdims=no value)\n",
    "- np.**percentile**(a, q percentage from 0 to 100, axis=None, out=None, overwrite_input=False, interpolation='linear', keepdims=False)\n",
    "\n",
    "\n",
    "- creating array a = np.arange(\n",
    "- array = **np.repeat( value, int for how many)** create array with value repeated folong givin int\n",
    "- **np.arange([start, ]stop, [step, ]dtype=None)** np.arrange(3,7,2) >> array([3,5])\n",
    "- to select different rows or columns use   **np.r_[ : 2 , 10, 12 : 22]**\n",
    "- np.timedelta64(1,'Y') used in calculation as 1 is int for count, Y is year, y s m w d \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "SQL\n",
    "===\n",
    "## SELECT\n",
    "Clauses:\n",
    "**WITH** table_name **AS** (subquery)\n",
    "- add several subquery as much as u want within 1 **WITH** just add **,** between them\n",
    "\n",
    "\n",
    "***SELECT*** col1, col2 or * as all **SELECT DISTINCT** return unique return result only\n",
    "- create **derived col**  use col +-/* col **AS** new_col_name \n",
    "- can use +-/* with dates to add days or hours **[DATE FUNCTIONS](https://www.postgresql.org/docs/9.1/functions-datetime.html)\n",
    "- assing alias name to any table.col with **AS** or just space and type it.\n",
    "- **COUNT** count non null rows / SELECT COUNT(use col or all*) AS count_col\n",
    "- **SUM(col)** count col values, using sum needs col not * / ignoring null\n",
    "- **MIN(col)** & **MAX(col)** same like SUM(col) aggregators-vertically- and ignoring null but if col is non numerical it return earlist or newest date or null in case of str\n",
    "- Average **AVG(col)** is aggergator and ignor null / in case ti count null as 0 SUM(col)/COUNT(col)\n",
    "- **TRUNC(value, int)** used with aggregators **TRUNC(AVG(col))** if int is +ve it truncates digits to the right of the decimal point, -ve  replaces digits to the left of the decimal point turnc(1.2345, 2)= 1.23 turnc(123.45, -2)= 100\n",
    "- **DATE_TRUN(*'day' or 'month' or 'year'or 'hour', 'minute'or 'second'*, col_date)** change only the givin level and keep top level data destinquished.\n",
    "- **DATE_PART(*'dow' or 'montrh'*, col_date)** 'dow' return 0 as sunday to 6 as satrday **neglet all the other levels**\n",
    "        change col with month_name to # DATE_PART('month', TO_DATE(col))\n",
    "- **CAST(col)** change formate to date and we can use **::date** after the concated col to reformate as date also.\n",
    "        CAST('2016-01-30' AS DATE), CAST('100' AS INTEGER) OR '2016-01-30'::DTAE , '100'::INTEGER\n",
    "- **CASE** **WHEN** col = 'str' or value **OR** col = 'str' or value **THEN** 'ur exp' **ELSE** optional 'ur exp' **END** **AS** 'drived col'  *workes like loop* & *could use aggregators in it*\n",
    "- **LEFT(col, # of element to pull)**, **RIGHT(col, #)**, **LENGTH(col)** length return #of element\n",
    "- **UPPER(name)** return col with all upper cases, **LOWER(col)** the oppiste\n",
    "- **POSITION('charc' in col)** & **STRPOS(col, 'charc')** both return the position # for a givin charc\n",
    "- **CONCAT(col1, 'charc', col2, 'charc2', ....)** or **col || 'charc' || col2**.\n",
    "- **TRIM('charc' FROM col or str)** remove sapes from beg & end| TRIM(col) will apply defualt for spaces\n",
    "- **REPLCAE(col or str, 'char u need to change or remove' , '')**\n",
    "- **SUBSTR(**'str' , start index, # of element to pull) same **SUBSTRING('str' from # for #)**\n",
    "- **COALESCE**(col, 'str') fill nulls in col with 'str' \n",
    "- SUM(col_1) **OVER** (**PARTITION BY** col_2 **ORDER BY** any_col ) **AS** total_running (we group by col_2 and order by any_col in each group\n",
    "- **ROW_NUMBER()** just assign order numbers for each row BUT **RANK()** gives row with the same value same rank (if we order by month all row in January will get same rank) **DENSE_RANK()** assign numbers in order\n",
    "- instead to write the window function many times for several queries, can assign it to an alias \n",
    "        **WINDOW** alias_name AS (**PARTITION BY** col_2 **ORDER BY** any_col ) \n",
    "        WINDOW comes between WHERE & GROUP BY\n",
    "- **LAG(col)** OVER (ORDER BY col) AS lag its pull values of pervious row\n",
    "- **LEAD(col)** OVER (ORDER BY col) AS lag its pull values of next row\n",
    "- identify what percentile **NTILE(# of bucket)** OVER (ORDER BY col_we_need) AS new_name\n",
    "\n",
    "***FROM*** table_name\n",
    "- could add subquery and treated as table to pull data from\n",
    "\n",
    "***JOIN*** table2 t2 or table2 AS t2 (**FROM** is left table & **Join** is right table)\n",
    "- JOIN is same INNER JOIN\n",
    "- LEFT JOIN is same LEFT OUTTER JOIN where(left is FROM and right is JOIN table\n",
    "- to join unmatched: FULL OUTER JOIN then use WHERE Table_A.column_name IS NULL OR Table_B.column_name IS NULL\n",
    "- Self join could be used using different alias to same table (\n",
    " \n",
    "***ON*** t2.col (PK) = table1.col (FK)\n",
    "- PK primary key \"unique value for each row\"\n",
    "- FK foreign key \"column that may have multiple rows with same value linked to PK\"\n",
    "- crow's foot means that related value is iterated in several rows\n",
    "- '+' sign only means that related data is showed 1 time only in the column\n",
    "- adding another equation with un joined table will act as a condition for joining data match that condition\n",
    "\n",
    "***WHERE*** boolean filter by logic or conditions(col = 'str') using = != > < or:\n",
    "- col **LIKE** '%str%' or **NOT** LIKE '%str%'\n",
    "- col **IN**('exact str1', exact sr2') or (value1, value2) or use **NOT** IN (....\n",
    "- condition **AND** condition (results must be True) \n",
    "- in case using **AND** for condition with same col use: col **BETWEEN** value or 'str' **AND** Value or 'str'\n",
    "- condition **OR** condition (combine all results form the conditions)\n",
    "- **IS NULL** to find null us IS as =/  WHERE col1 **IS** NULL or **IS NOT**\n",
    "- doesn't support aggregation like SUM(col) >= value\n",
    "- Using ** **AND** to add extra conditions\n",
    "        ON table1.col1 = t2.col2\n",
    "        AND t1.date1 > t2.date2\n",
    "\n",
    "**GROUP BY** group multiple date and gives its aggregation\n",
    "- aggregation in SELECT and data that cant be agregated in GROUP BY\n",
    "- The GROUP BY always goes between WHERE and ORDER BY.\n",
    "- Any column in the SELECT statement that could not be aggregated, will be in the GROUP BY clause.\n",
    "\n",
    "**HAVING** boolean filters like WHERE but with using aggregation\n",
    "- comes after GROUP BY\n",
    "\n",
    "***ORDER BY*** first col is the primary, it order col1 then order related results of col2.. after finish start second result of col1:\n",
    "- col1, col2 **DESC**, col3 (default is ascending a-z, lowest to highest, earliest to latest)\n",
    "\n",
    "***LIMIT*** # of result to display;\n",
    "\n",
    "# imp\n",
    "- when selecting several col to display with same name, must assign alias to each one to display\n",
    "- **ON** logic applies 1st then **WHERE** excute its logic on the results of **ON**, so may *WHERE* remove data we need from *ON* table, we can apply **AND** after *ON* instead of *WHERE*.\n",
    "- JOIN multiple tables by write extra JOIN t# ON logic and so on another query\n",
    "- **subquery** could be used as col_name, table_name or even a value ex: WHERE value = (SELECT ..FROM...)\n",
    "- limitations for speed does not work with aggregation as system preform the ful aggregation then limit the results\n",
    "- to speedup query:\n",
    "        - if using dates : try using small durations\n",
    "        - test system first and try to put limit inside the subquery not outer\n",
    "        - try to set the aggreagtions inside subquery with limit\n",
    "        - pre aggregate for the requested data tables in a subquery before joinning it with another table\n",
    "        - use **EXPLAIN** before SELECT print the order and time use of my query\n",
    "\n",
    "[The SQL UNION Operator](https://www.w3schools.com/sql/sql_union.asp) **UNION** & **UNION ALL** add col to each other\n",
    "- UNION collect unique data only, UNION ALL add all data even if iterated.\n",
    "\n",
    "        SELECT city FROM table1\n",
    "        WHERE col1 = 'Germeny' (example)\n",
    "        UNION\n",
    "        SELECT city FROM Table2\n",
    "        WHERE col1 = 'Germany'\n",
    "        ORDER BY city\n",
    "\n",
    "[Cross Join in SQL](https://www.w3resource.com/sql/joins/cross-join.php) **CROSS JOIN*\n",
    "- first table multiplied by the number of rows in the second table if no WHERE clause, if WHERE used, it will be like INNER JOIN\n",
    "\n",
    "        SELECT foods.item_name,foods.item_unit, company.company_name,company.company_city\n",
    "        FROM foods \n",
    "        CROSS JOIN company;\n",
    "        \n",
    "[STRING FUNC](https://www.postgresql.org/docs/9.1/functions-string.html)\n",
    "\n",
    "\n",
    "**MORE**\n",
    "- **INSERT INTO** table_name (column1, column2, column3, ...)  **VALUES** (value1, value2, value3, ...);\n",
    "- to select NULL SELECT * FROM t1 WHERE col1 IS NULL\n",
    "- **UPDATE** table_name **SET** column1 = value1, column2 = value2, ..  **WHERE** condition;\n",
    "- **DELETE** **FROM** table_name **WHERE** condition (DELETE FROM col  will delete all col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Standard and other important lib:\n",
    "=================================\n",
    "\n",
    "- csv: very convenient for reading and writing csv files\n",
    "- collections: useful extensions of the usual data types including OrderedDict, defaultdict and namedtuple\n",
    "- random: generates pseudo-random numbers, shuffles sequences randomly and chooses random items\n",
    "- string: more functions on strings. This module also contains useful collections of letters like string.digits (a string containing all characters with are valid digits).\n",
    "- re: pattern-matching in strings via regular expressions\n",
    "- math: some standard mathematical functions\n",
    "- os: interacting with operating systems\n",
    "- os.path: submodule of os for manipulating path names\n",
    "- sys: work directly with the Python interpreter\n",
    "- json: good for reading and writing json files (good for web work)\n",
    "\n",
    "\n",
    "- IPython - A better interactive Python interpreter\n",
    "- requests - Provides easy to use methods to make web requests. Useful for accessing web APIs.\n",
    "- Flask - a lightweight framework for making web applications and APIs.\n",
    "- Django - A more featureful framework for making web applications. Django is particularly good for designing complex, content heavy, web applications.\n",
    "- Beautiful Soup - Used to parse HTML and extract information from it. Great for web scraping.\n",
    "- pytest - extends Python's builtin assertions and unittest module.\n",
    "- PyYAML - For reading and writing YAML files.\n",
    "- NumPy - The fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object and useful linear algebra capabilities.\n",
    "- pandas - A library containing high-performance, data structures and data analysis tools. In particular, pandas provides dataframes!\n",
    "- matplotlib - a 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments.\n",
    "- ggplot - Another 2D plotting library, based on R's ggplot2 library.\n",
    "- Pillow - The Python Imaging Library adds image processing capabilities to your Python interpreter.\n",
    "- pyglet - A cross-platform application framework intended for game development.\n",
    "- Pygame - A set of Python modules designed for writing games.\n",
    "- pytz - World Timezone Definitions for Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantitative** data takes on numeric values that allow us to perform mathematical operations (like the number of dogs).\n",
    "\n",
    "- **Continuous** data can be split into smaller and smaller units, and still a smaller unit exists. An example of this is the age of the dog - we can measure the units of the age in years, months, days, hours, seconds, but there are still smaller units that could be associated with the age (Height, Age, Income).\n",
    "- **Discrete data** only takes on countable values. The number of dogs we interact with is an example of a discrete data type (Pages in a Book, Trees in Yard, Dogs at a Coffee Shop)\n",
    "\n",
    "\n",
    "**Categorical** are used to label a group or set of items (like dog breeds - Collies, Labs, Poodles, etc.).\n",
    "\n",
    "- **Categorical Ordinal** data take on a ranked ordering (like a ranked interaction on a scale from Very Poor to Very Good with the dogs) (Letter Grade, Survey Rating).\n",
    "- **Categorical Nominal** data do not have an order or ranking (like the breeds of the dog)(Gender, Marital Status, Breakfast Items).\n",
    "\n",
    "\n",
    "**Categorical** data is analyzed usually be looking at the counts or proportion of individuals that fall into each group\n",
    "\n",
    "## Analyzing Quantitative Data:\n",
    "\n",
    "1- **Descriptive statistics** is about describing our collected data:\n",
    "\n",
    "    1- Measures of Center (mean, median 'center of sorted list' , mode 'most frequesnt value')\n",
    "    2- Measures of Spread (range, Interquartile Range (IQR), Standard Deviation, Variance)\n",
    "    3- The Shape of the data(left skewed (small bin left & high bins right), right skewed, Symmetric or normal \n",
    "    4- Outliers (data point that fall very far that other data values)\n",
    "- in Symmetric (mode = median = mean), left (mode > median > mean), right (mean > median)\n",
    "- dealing with outliers (Plot your data to identify if you have outliers, Handle outliers -fix, remove, keep- , for normal distribution use mean & std , for skewed data use 5 num summary\n",
    "\n",
    "- **random variable X or Y or Z** is a placeholder for the possible values of some process (like column is holder of entire set of possible values)\n",
    "- $xn$ lower case x , we notate it as individual random variable value and n is a subscript as order of observing the outcome. X = x1, x2, x3....., xn) as n count of rows\n",
    "- P(X>20)? P is probability, this notation represents the probability occuring the outcome more than 20, if answer is 2 time of 5 possibalities, then *2 of 5 or 40%*\n",
    "- **Sigma Σ** upper case letter, for summation of multiple values\n",
    "- $ \\sum\\limits_{i = 1}^3 x_i$ sum of x values started from i=1 to endof  the top = 3\n",
    "- **Pi Π**  multiply all of our values together .\n",
    "- **LONG S ∫** aggregate continuous values is with something known as integration (a common technique in calculus)\n",
    "- **x bar** $ x¯ = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i $ sum of all values (n) and divide by n to calculate mean $x¯$ or $y¯$ or $z¯$ population paremeter for mean is $\\mu$ μ\n",
    "- $  {\\frac{\\sum\\limits_{j=1}^ny_j}{n}}  = y¯ = x¯ = {\\frac{\\sum\\limits_{i=1}^nx_i}{n}} $  = mean\n",
    "- **5 number summary** consist of 5 values\n",
    "        Minimum: The smallest number in the dataset\n",
    "        Q1: value 25% of the data fall below (median of data below Q2)\n",
    "        Q2: value 50% of the data (median)\n",
    "        Q3: value 75% of the data fall below (median of data above Q2)\n",
    "        Maximum: The largest value in the dataset\n",
    "- **Range** difference between maximum & minimum.\n",
    "- **IQR The interquartile range** the difference between Q3 & Q1.\n",
    "- Higher mean does not mean higher std\n",
    "- **Standard deviation** the average distance of each observation from the mean : $\\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2} $\n",
    "        calculate mean $x¯$\n",
    "        distance of each observation from the mean $x_i - x¯ $\n",
    "        squaring them all and get the mean (variance) is σ²\n",
    "        square root the variance\n",
    "- **variance** mean of the squared difference of each observation  $x_i - x¯ $ final = $ \\frac{1}{n}\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2 $ as sum of the difference divided by n (where it is sample set)\n",
    "- The \"Population Standard Deviation\": divid by N\n",
    "- The \"Sample Standard Deviation\":\tdivid by n-1\n",
    "\n",
    "\n",
    "2- **Inferential Statistics** is about using our collected data to draw conclusions to a larger population:\n",
    "\n",
    "    1- Population - our entire group of interest.\n",
    "    2- Parameter - numeric summary about a population (actual) it could be mean \n",
    "    3- Sample - subset of the population\n",
    "    4- Statistic - numeric summary about a sample, it could be mean\n",
    "\n",
    "## Probalbility \n",
    "- fair coin (equal chance for each side) \n",
    "- Loaded coin (1 element has better chance than others)\n",
    "- probaility of T = P(not H)=P(T)=$P{\\lnot H} $ (sign $¬ $ means not)\n",
    "- Across multiple coin flips, we have the probability of seeing n heads as ${P(H)^n} $\n",
    "- The probability of any event must be between 0 and 1\n",
    "- **Binomial distribution** only with 2 outcomes like coin flip which is an independent = n!/ (n-k)! k! * p^k * (1-p)^(n-k) where n is the number of events, k is the number of \"successes\", and - p is the probability of \"success\" $P(X = x) = \\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$\n",
    "- **Conditional Probability** the outcome of one event depends on an earlier event \n",
    "- $P(A|B)= \\frac{P(A\\text{ }\\cap\\text{ }B)}{P(B)}$ where | represents \"given\", $\\cap$ represents \"and (∩)\" the outcome of A depend on the outcome of B & the combined probability P(B, A)= P(B) P(A|B)\n",
    "- **Bayes rule** (prior probability) (evidence) gives (posterior probablilty)\n",
    "    - say we have grouped a school into smoker & non smoker (population is school)\n",
    "    - prior probability is P(smk) = 0.2 with evidence of who made smoke test P(+|smk) = 0.9 & P(-| smk)=0.1 \n",
    "    - and other perior is P(non) = 0.8 with evidence of who made smoke test P(+| non) = 0.15 & P(-| non) = 0.85\n",
    "    - to get total smokers, we (combin or join) probability with evidence by P(+, smk)= P(smk) P(+|smk) & P(+ , non)= P(non) P(+|non)\n",
    "    - then sum the result will name NORMALIZER P(+)\n",
    "    - divide the (combined or joined) result by the normalizer as P(smk| +) = P(+|smk)/P(+) and P(non| +) = P(+|non)/P(+) then we get the posterior probablility of smoker with + test.\n",
    "    - here the probability changed from P(+|smk) to P(smk|+) as we are getting the data from and answering quest \n",
    "    - as follow $ P(A|B ) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$ in our case P(smk|+) = P(+, smk)/P(+) = P(+|smk) P(smk)/P(+)\n",
    "\n",
    "- **Normal distribution**\n",
    "- f(x) = (x-μ)^2 / σ² \n",
    "- f(x) = function results that use x as input\n",
    "- larger variances wider shape\n",
    "\n",
    "- **Sampling distribution** is the distribution of a statistic(any statistic: such as mean) for different samples\n",
    "    - The variance of the original set = mean(1-mean)\n",
    "    - The variance of the sampling distribution(mean) for the proportion computed with sample size = mean(1-mean) / size\n",
    "    - The sampling distribution decreases its variance depending on the sample size used.\n",
    "    - the sampling distribution of the sample mean  ${\\bar{X}} $ = ${\\frac{\\sigma^2}{n}} $\n",
    "- **Low or large number** larger sample size the closer sample mean to the population mean.\n",
    "- **Central Limit Theorem** with a large enough sample size to compute the sampling distribution, the mean will be normally distributed (it could works for means & proportions and its differences)\n",
    "- **Bootstrapping** resampling the same sample over and over gives good results to provide an estimate of the sampling distribution: like np.random.choice(a, replace = True)\n",
    "- **confidence intervals** a range of values so defined that there is a specified probability that the value of a parameter lies within it (confidence intervals are about parameters in a population, and not about individual observations)\n",
    "    - **Statistical significance** Using confidence intervals and hypothesis testing, you are able to provide statistical significance in making decisions.\n",
    "    - Increasing your sample size will decrease the width of your confidence interval.\n",
    "    - Increasing your **confidence level** (say 95% to 99%) will increase the width of your confidence interval.\n",
    "    - The confidence interval **width** as the difference between your upper and lower bounds of your confidence interval.\n",
    "    - The margin of error **MOE** is half the confidence interval width, and the value that you add and subtract from your sample estimate to achieve your confidence interval final results\n",
    "    \n",
    "- **Practical significance** takes into consideration other factors of your situation that might not be considered directly in the results of your hypothesis test or confidence interval. Constraints like space, time, or money are important in business decisions.\n",
    "\n",
    "- **hypotheses testing** \n",
    "- The H0 is true before you collect any data, usually states there is no effect or that two groups are equal, H0 contains an equal sign of some kind - either =, ≤, or ≥. \n",
    "- H1 is what we would like to prove to be true, contains the opposition of the null - either != , >, or \n",
    "- The H_0H and H1 are competing, non-overlapping hypotheses, \n",
    "- A Type I Error α (symbol $\\alpha$ - false positives)is worest error when the alternative is chosen, but the null is actually true.\n",
    "- A Type II Error β (symbol $\\beta$ - false negatives) when the null is chosen, but the alternative is actually true.\n",
    "\n",
    "- 1st method using CI and bootstrap to check if we fail to reject or will actually reject the H0 (null)\n",
    "- 2nd get std of our sampling distribution and create normal distribution using value of null that is closest to alternative and compare our sampling statistice to our normal dis.\n",
    "\n",
    "- P-value is the probability of observing _statistic_ (or one more extreme in favor of the alternative) if the null hypothesis is true, where we compare normal dis to sample statistic (2nd method) and we shade after sample statistic far from null.\n",
    "    - if null_mean <= value : p-value is all area after upper value (sample mean)\n",
    "    - if null_mean >= value : p-value is all area before upper value (sample mean)\n",
    "    - in case null = value : p-value is all area after sample mean and below null - diff of sample_mean & null_mean or the oppiste in case sample_mean less than null mean.\n",
    "    - if P-value is small its mean that its less likely to observe our static from the null ( large of a p-value suggests that we do not have evidence to reject the null)\n",
    "    - if p-value <= $\\alpha$ we reject the null\n",
    "    - Bonferroni correction is used to correct type I error in case testing several times as $\\alpha$ / test_count\n",
    "    - when sample sizes are really large, everything appears statistically significant\n",
    "\n",
    " \n",
    "- Regression:\n",
    "- **simple linear regression** we compare two quantitative variables to one another, such as ploting scatter where y axis (one we interested in predicting) named the response of dependent variable, x axis (used to predict the response) named explanatory or independent variable.\n",
    "- **Scatter plots** have correlation coefficient (r) which gives u strength (strong: close point to each other, moderate, weak) and direction (positive: going up togather, negitive) and numerice value between 1 : -1 close to 1 or -1 means strong and positive value means positive direction vice versa.($\n",
    "r = \\frac{\\sum\\limits_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2}\\sqrt{\\sum(y_i - \\bar{y})^2}}$)\n",
    "- linear used **intercept** (predict respone value when explanatory =0) b0 for sample & $\\beta0$ for parmeter, and **slope** b1, beta1 the expected response for each unit increase in explanatory\n",
    "- $\\hat{y} = b_0 + b_1x_1$ where y1 hate is predicted value of the response from the line,  and y1 represent the actual value not on the line ($b_1 = r\\frac{s_y}{s_x}$ & $b_0 =\\bar{y} - b_1\\bar{x}$ \n",
    "- to make this line use least squares $ \\sum\\limits_{i=1}^n(y_i - \\hat{y_i})^2 $\n",
    "- **-squared value** is the square of the correlation coefficient (the amount of variability in the response variable that can be explained by the x-variable in our model. In general, the closer this value is to 1, the better our model fits the data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

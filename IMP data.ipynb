{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First\n",
    "=====\n",
    "\n",
    "Second\n",
    "------\n",
    "\n",
    "# Header 1\n",
    "## Header 2\n",
    "### Header 3\n",
    "\n",
    "[for more data](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) here **i'am** _just_ __looking__ around \n",
    "[list of magic words](https://ipython.readthedocs.io/en/stable/interactive/magics.html)\n",
    "$$\n",
    "y = \\frac{a}{b+c}\n",
    "$$\n",
    "> **Another test:** Click on this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Jupyter:\n",
    "=======\n",
    "- jupyter nbconvert --to html notebook.ipynb   \\convert html to .ipynb [check doc](https://nbconvert.readthedocs.io/en/latest/usage.html)\n",
    "- jupyter nbconvert notebook.ipynb --to slides \\convert and immediately see jupyter nbconvert notebook.ipynb --to slides --post serve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Conda** :\n",
    "\n",
    "- conda upgrade conda\n",
    "- conda upgrade --all\n",
    "- conda create –n project_name python=3\n",
    "- conda create -n env_name list of packages \\ex: conda create -n data python=3.6 numpy pandas\n",
    "- conda env export > environment.yaml\n",
    "- conda env create -f environment.yaml\n",
    "- conda env remove -n env_name\n",
    "- activate project_name \\deactivate\n",
    "- conda env list\n",
    "- conda install numby panda matplotlib\n",
    "- conda install jupyter notebook\n",
    "- conda remove package_name=1.0.2 \n",
    "- conda update package_name\n",
    "- conda update –all\n",
    "- conda list\n",
    "- conda search *search_term*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Panda:\n",
    "======\n",
    "\n",
    "- **df = pd.read_csv**('student_scores.csv', sep=',' , header = 0 or None or any int, names = create list with columns, index_col = 'column name to be the index' or more than one by creating list , index = False to cancel the index num if u have one,  ) start reading\n",
    "- **df.head(5)**   return first 5 rows along with the header, or set how much to return.\n",
    "- **df.tail(5)**   return last 5 rows along with the header, or to set how much to return.\n",
    "- **df.shape**    return tuple ( rows count, columns count).\n",
    "- **df.dtypes**    return the data type of each columns(bool, int, float, object).\n",
    "        use type(df.column[1]) or type(df['column'][1]\n",
    "        \n",
    "- **df.copy(*deep* = *True* default or *False* both copy reflect any change in one of them)**\n",
    "- **type(df['column name'][0])** return the excatly data type of data(str, ....).\n",
    "- **df.info()**  return summary of datafram and number of non-null values in each column.\n",
    "- **df.nunique()**    return number of unique values in each column & **df.unique()** return list of that values.\n",
    "- **df.column_name.nunique()** return the no of unique in singal column (write without ' ').\n",
    "- **df.describe()**    return seful descriptive statistics for each column of data:\n",
    "            df.describe()['min or 25% or 50% or 75% or max] return only required value\n",
    "            use list(df.describe()['count','mean', 'std' ,'min' : 'max']) or ['min': ] to make a list with values\n",
    "            use df.describe()['min' : 'min'] if there are servral columns aslo add ['column'] to select specific 1\n",
    "- **df_selecting = df.iloc**[ form row index : to row index , from column index : to column index] \n",
    "- **df_selecting = df.loc**[ form row index : to row index , from column 'name' : to column 'name' ]\n",
    "- to select different rows or columns use   **np.r_[ : 2 , 10, 12 : 22]**    but import numpy first.\n",
    "- **df.isnull()** print boolean for all df null True / **df.isnul().sum()** for total counts\n",
    "        df.isnull().any(axis=1).sum() total count of rows with null values \n",
    "        any() return boolean results / axis=1 to select the rows instead of columns\n",
    "- **df.columns** = new_labels to change the columns name .\n",
    "        check if 2 df are the same columns for appending (df_1.columns == df_2.columns).all() one result True or False / or df_1.columns == df_2.columns result a list of True or False for each column\n",
    "- SWITCH rows and columns **df.transpose()** \n",
    "- change df to serise by using **iloc[:, :]**\n",
    "\n",
    "sample to print the columns with its index# for slicing:\n",
    "\n",
    "> - for i, v in enumerate(df.columns):\n",
    ">     - print(i, v)\n",
    "\n",
    "- Fill the null values with mean:\n",
    "            1- mean = df['column name'].mean().\n",
    "            2- df['column nam'].fillna(mean, inplace = True) \n",
    "                or assign it to the original df['coumn name'] = df['coulmn name'].fillna(mean)\n",
    "\n",
    "- **df.duplicates()**    return all row with True or False (check that all data are the same):\n",
    "            **df.duplicated().sum()** or **sum(df.duplicated())** return total identical rows.\n",
    "- **df.drop_duplicates**(inplace = True) to del the identical rows.\n",
    "- df['column time'] = **pd.to_datetime**(df['column time']) .\n",
    "- **pd.to_numeric**(*arg*, *downcast*={‘integer’, ‘signed’, ‘unsigned’, ‘float’}, *errors* = {‘ignore’, ‘raise’, ‘coerce’})\n",
    "        or df_18['column'].astype(int) pr astype(float)\n",
    "- drop rows or columns **df.drop**(*labels*=list, *axis*=0, *index*=None, *columns*=list, *level*=None, *inplace*=False)\n",
    "        df.drop(selected_df.index, inplace=True)\n",
    "- drop null rows **df.dropna**(*axis*=0 'index or 1 'column', *how*='any' with null or 'all' only row with all null , *thresh*=None or drop at least # of null in each row or column, subset=None or define which column to look at, *inplace*=False)\n",
    "\n",
    "    *Plotting*:\n",
    "    ----------\n",
    "- **df.hist() ;** figsize=(int, int) optional end by **;** to view histograms for all the numerical columns, or **df['column name'].hist();** to view a spcifice column.\n",
    "> can use **df['column name'].plot(kind = 'hist');**  kind = hist, bar, pie, scatter, box / title='str' /color= 'red' or ['red', 'blue'] / alpha = 0.7\n",
    "- **pd.plotting.scatter_matrix(df, figsize=(15,15));** to watch matrix of all numercial columns (for quick insight).\n",
    "- close results needs **df.plot.line()** show the difference in lines and \n",
    "- 2 columns **df.plot(x= column name , y= other coulmn name , kind = 'scatter')**\n",
    "- seperate group of df(by assign each to var) and use them visualise and compare to another data in df\n",
    "- for non numercial columns, use **df['column name'].value_counts()** to count each repeated value and return it.\n",
    "> can call plot directly **df['column name'].value_counts().plot(kind = 'bar', figsize = (8 ,8));**\n",
    "    - to set index: ind = **df['column u need data'].value_counts().index** used in plotting\n",
    "    - example: **df['column name'].value_counts()[ind].plot(kind = 'bar', figsize = (8 ,8));**\n",
    "\n",
    "\n",
    "   *Create your own data frame:*\n",
    "    ------------------------------\n",
    "- df_m = **df[df['from column name'] == 'm']** assigning specific data or **df.query(' from column name == \"m\" ' )**:\n",
    "            df.query('column name > {} '.format(df[''].median()) example of using format() in query\n",
    "            df.query('column in [list of several values])\n",
    "- mask = **df['from column name'] == 'm'**  return serious of boolean weather all columns equal to certain value.\n",
    "- **df_m = df[mask]** is another way to assign all True from mask to df_m.\n",
    "- last_month = **df[df['week']>= '02-01-2018']**  assinging last month data as example......\n",
    "- **last_month.append(df.sum(numeric_only=True), ignore_index=True)**   here sum the total\n",
    "    - or **df.iloc[int: , 1: ].sum()** here we selected the required data and we skip the non numerical\n",
    "- using **df[df['column'] == df['column'].min()]** or max() or sum() mean() median() count().\n",
    "- by index **df[ df.index >= 'value u need to start' ]**\n",
    "- return duplicated row **df.loc[ df.duplicated() , : ]** or  **df.loc[df['data in column'].unnique() , : ]**\n",
    "- add new or update columns: **df['new column'] = array** , **array = np.repeat( value, int for how many)**\n",
    "\n",
    "- [rename](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html) column name **df.rename**(**index**=str, **columns**={ from \"A\": to \"a\"}, **axis**='columns or index' **inpalce** = True)\n",
    "        df.rename(columns=lambda x: x[:10]+'str') example\n",
    "- apply func to all row or column **df.apply(func, axis= {0 or ‘index’, 1 or ‘columns’} default 0)**\n",
    "- Saving file **df.to_csv('Path' , sep = ',' , index = False)** \n",
    "\n",
    "    *Grouping*:\n",
    "    -----------\n",
    "- **df.groupby('column').mean()** column will be the index\n",
    "- **df.groupby(['column1', 'column2'])** or **(as_index = False)** to cancel column to be index, or **groupby(...)['display specific column']**:\n",
    "            or df.groupby('column to grouped').mean().column_name to be returned along.\n",
    "\n",
    "    *CUTTing*:\n",
    "    ---------- \n",
    "divid column into pieces as per givin labels\n",
    "- set bins = [min , 25% , 50% , 75% , max ]\n",
    "- set labels=[ ' G1 ' , ' G2 ' ,' G3 ' , ' G4 '  ]\n",
    "- df['new column name'] = **pd**.cut(df['column needed'] , bins , labels = labels_list)\n",
    "\n",
    "##[String handling](https://pandas.pydata.org/pandas-docs/stable/api.html#string-handling):\n",
    "- **df.str.extract**('similar str to original and add expression of what u need' , *expand*= True return df or False Series  \n",
    "- **df.str.contains('/')** return all data that contain a given charachters\n",
    "\n",
    "expressions:\n",
    "- \\w any single letter, digit or underscore\n",
    "- \\W any charchter not part of \\w\n",
    "- \\s space, newline, tab, return\n",
    "- \\S any character not part of \\s \n",
    "- \\t tab - \\n newline - \\r return\n",
    "- \\d decimal digit 0-9 & use \\d+ to collect next numbers\n",
    "- ^  pattern at the start of the str - **dollar signe** pattern at the end of str\n",
    "- [abc] match a or b or c\n",
    "- [a-zA-Z0-9] (a to z) or (A to Z) or (0 to 9)\n",
    "\n",
    "\n",
    "- [concat](https://pandas.pydata.org/pandas-docs/stable/merging.html#concatenating-objects) pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True)\n",
    "- [merge](https://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging) pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True,suffixes=('_x', '_y'), copy=True, indicator=False,validate=None)\n",
    "        combined_df = pd.merge(left_df, right_df, left_on=  , right_on = )\n",
    "                or = df_left.merge(df_right, left_on , right_on)\n",
    "\n",
    "[Check this 1](http://nbviewer.jupyter.org/format/slides/github/jorisvandenbossche/2015-PyDataParis/blob/master/pandas_introduction.ipynb#/)\n",
    "\n",
    "[Check this 2](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python)\n",
    "\n",
    "[Sammary sheet](https://www.dataquest.io/blog/pandas-cheat-sheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "matplotlib\n",
    "========\n",
    "\n",
    "- set plot title **plt.title('str')**\n",
    "- set X name **plt.xlabel('title' , fontsize = 12)** / Y name **plt.ylabel('title', 12)**\n",
    "- customizing **plt.bar**(*left*: [ticks loc seq] , *height*: [bars values], *width*: widths bar , *bottom*: y coor, *color*, *edgecolor* , *linewidth* , *tick_label*: tick label, *orientation*: bars orientation , *log*: boolean)\n",
    "        left = ['1', '5'] as 2 locations or [1,5] as 5 locations then inster location names by tick_label\n",
    "        df2 =df.groupby('column1').column2.mean()\n",
    "        plt.bar(df2.index , df2 )\n",
    "        plt.subplots(figsize=(8, 5))\n",
    "\n",
    "- x ticks names **plt.xticks(ticks, label )** ticks=list of positions 1,2,3 or 3,1,2 labels= list of label for location:\n",
    "        locs, labels = xticks()  # Get locations and labels\n",
    "        xticks(ticks, [labels])  # Set locations and labels\n",
    "- line plot **plt.plot( x: seq , y: seq )**\n",
    "\n",
    "example:\n",
    "---------\n",
    "### plot bars\n",
    "- red_bars = plt.bar(ind, red_proportions, width, color='r', alpha=.7, label='Red Wine')\n",
    "- white_bars = plt.bar(ind + width, white_proportions, width, color='w', alpha=.7, label='White Wine')\n",
    "        red_proportion is serise, if not use red_proportion.column that u need\n",
    "        ind is array of num inorder to +width\n",
    "\n",
    "### title and labels\n",
    "- plt.sunplots(figsize = (8 width ,5 hieght)\n",
    "- plt.ylabel('Proportion')\n",
    "- plt.xlabel('Quality')\n",
    "- locations = ind + width/2  # xtick locations\n",
    "- labels = ['3', '4', '5', '6', '7', '8', '9']  or **serise.index** # xtick labels\n",
    "- plt.xticks(locations, labels)\n",
    "\n",
    "### legend\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Numpy:\n",
    "======\n",
    "\n",
    "- creating array a = np.array(\n",
    "- array = **np.repeat( value, int for how many)** create array with value repeated folong givin int\n",
    "- **np.arange([start, ]stop, [step, ]dtype=None)** np.arrange(3,7,2) >> array([3,5])\n",
    "- to select different rows or columns use   **np.r_[ : 2 , 10, 12 : 22]**\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "SQL\n",
    "===\n",
    "## SELECT\n",
    "Clauses:\n",
    "\n",
    "***SELECT*** col1, col2 or * as all **SELECT DISTINCT** return unique return result only\n",
    "- create **derived col**  use col +-/* col **AS** new_col_name \n",
    "- can use +-/* with dates to add days or hours **[DATE FUNCTIONS](https://www.postgresql.org/docs/9.1/functions-datetime.html)\n",
    "- assing alias name to any table.col with **AS** or just space and type it.\n",
    "- **COUNT** count non null rows / SELECT COUNT(use col or all*) AS count_col\n",
    "- **SUM(col)** count col values, using sum needs col not * / ignoring null\n",
    "- **MIN(col)** & **MAX(col)** same like SUM(col) aggregators-vertically- and ignoring null but if col is non numerical it return earlist or newest date or null in case of str\n",
    "- Average **AVG(col)** is aggergator and ignor null / in case ti count null as 0 SUM(col)/COUNT(col)\n",
    "- **DATE_TRUN(*'day' or 'month' or 'year'or 'hour', 'minute'or 'second'*, col_date)**\n",
    "- **DATE_PART(*'dow' or 'montrh'*, col_date)** 'dow' return 0 as sunday to 6 as satrday **but year are not being consider**\n",
    "\n",
    "***FROM*** table_name\n",
    "- could add subquery and treated as table to pull data from\n",
    "\n",
    "***JOIN*** table2 t2 or table2 AS t2 (**FROM** is left table & **Join** is right table)\n",
    "- JOIN is same INNER JOIN\n",
    "- LEFT JOIN is same LEFT OUTTER JOIN\n",
    "\n",
    "***ON*** t2.col (PK) = table1.col (FK)\n",
    "- PK primary key \"unique value for each row\"\n",
    "- FK foreign key \"column that may have multiple rows with same value linked to PK\"\n",
    "- crow's foot means that related value is iterated in several rows\n",
    "- '+' sign only means that related data is showed 1 time only in the column\n",
    "\n",
    "***WHERE*** boolean filter by logic or conditions(col = 'str') using = != > < or:\n",
    "- col **LIKE** '%str%' or **NOT** LIKE '%str%'\n",
    "- col **IN**('exact str1', exact sr2') or (value1, value2) or use **NOT** IN (....\n",
    "- condition **AND** condition (results must be True) \n",
    "- in case using **AND** for condition with same col use: col **BETWEEN** value or 'str' **AND** Value or 'str'\n",
    "- condition **OR** condition (combine all results form the conditions)\n",
    "- **NULL** to find null us IS as =/  WHERE col1 **IS** NULL or **IS NOT**\n",
    "- doesn't support aggregation like SUM(col) >= value\n",
    "\n",
    "**GROUP BY** group multiple date and gives its aggregation\n",
    "- aggregation in SELECT and data that cant be agregated in GROUP BY\n",
    "- The GROUP BY always goes between WHERE and ORDER BY.\n",
    "- Any column in the SELECT statement that could not be aggregated, will be in the GROUP BY clause.\n",
    "\n",
    "**HAVING** boolean filters like WHERE but with using aggregation\n",
    "- comes after GROUP BY\n",
    "\n",
    "***ORDER BY*** first col is the primary, it order col1 then order related results of col2.. after finish start second result of col1:\n",
    "- col1, col2 **DESC**, col3 (default is ascending a-z, lowest to highest, earliest to latest)\n",
    "\n",
    "***LIMIT*** # of result to display;\n",
    "\n",
    "# imp\n",
    "- when selecting several col to display with same name, must assign alias to each one to display\n",
    "- **ON** logic applies 1st then **WHERE** excute its logic on the results of **ON**, so may *WHERE* remove data we need from *ON* table, we can apply **AND** after *ON* instead of *WHERE*.\n",
    "- JOIN multiple tables by write extra JOIN t# ON logic and so on\n",
    "\n",
    "[The SQL UNION Operator](https://www.w3schools.com/sql/sql_union.asp) **UNION** & **UNION ALL** add col to each other\n",
    "- UNION collect unique data only, UNION ALL add all data even if iterated.\n",
    "\n",
    "        SELECT city FROM table1\n",
    "        WHERE col1 = 'Germeny' (example)\n",
    "        UNION\n",
    "        SELECT city FROM Table2\n",
    "        WHERE col1 = 'Germany'\n",
    "        ORDER BY city\n",
    "\n",
    "[Cross Join in SQL](https://www.w3resource.com/sql/joins/cross-join.php) **CROSS JOIN*\n",
    "- first table multiplied by the number of rows in the second table if no WHERE clause, if WHERE used, it will be like INNER JOIN\n",
    "\n",
    "        SELECT foods.item_name,foods.item_unit, company.company_name,company.company_city\n",
    "        FROM foods \n",
    "        CROSS JOIN company;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Standard and other important lib:\n",
    "=================================\n",
    "\n",
    "- csv: very convenient for reading and writing csv files\n",
    "- collections: useful extensions of the usual data types including OrderedDict, defaultdict and namedtuple\n",
    "- random: generates pseudo-random numbers, shuffles sequences randomly and chooses random items\n",
    "- string: more functions on strings. This module also contains useful collections of letters like string.digits (a string containing all characters with are valid digits).\n",
    "- re: pattern-matching in strings via regular expressions\n",
    "- math: some standard mathematical functions\n",
    "- os: interacting with operating systems\n",
    "- os.path: submodule of os for manipulating path names\n",
    "- sys: work directly with the Python interpreter\n",
    "- json: good for reading and writing json files (good for web work)\n",
    "\n",
    "\n",
    "- IPython - A better interactive Python interpreter\n",
    "- requests - Provides easy to use methods to make web requests. Useful for accessing web APIs.\n",
    "- Flask - a lightweight framework for making web applications and APIs.\n",
    "- Django - A more featureful framework for making web applications. Django is particularly good for designing complex, content heavy, web applications.\n",
    "- Beautiful Soup - Used to parse HTML and extract information from it. Great for web scraping.\n",
    "- pytest - extends Python's builtin assertions and unittest module.\n",
    "- PyYAML - For reading and writing YAML files.\n",
    "- NumPy - The fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object and useful linear algebra capabilities.\n",
    "- pandas - A library containing high-performance, data structures and data analysis tools. In particular, pandas provides dataframes!\n",
    "- matplotlib - a 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments.\n",
    "- ggplot - Another 2D plotting library, based on R's ggplot2 library.\n",
    "- Pillow - The Python Imaging Library adds image processing capabilities to your Python interpreter.\n",
    "- pyglet - A cross-platform application framework intended for game development.\n",
    "- Pygame - A set of Python modules designed for writing games.\n",
    "- pytz - World Timezone Definitions for Python\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
